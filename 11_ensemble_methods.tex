\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage[export]{adjustbox}
\usepackage{geometry}
\geometry{a4paper, margin=1in}

\title{Ensemble Methods}
\author{}
\date{}

\begin{document}

\maketitle

\section{Introduction to Ensemble Methods}

\begin{itemize}
    \item \textbf{Definition:} Ensemble methods combine predictions from multiple models to improve overall performance and robustness.
    \item \textbf{Purpose:} Reduce errors by leveraging the strengths of various models and compensating for their weaknesses.
    \item \textbf{Real-World Example:} A jury making a decision collectively often outperforms individual juror decisions.
\end{itemize}

\section{Types of Ensemble Methods}

\subsection{Bagging (Bootstrap Aggregating)}

\begin{itemize}
    \item \textbf{Definition:} A method that creates multiple versions of a dataset by random sampling with replacement and trains a separate model on each.
    \item \textbf{Mathematical Detail:}
    \begin{itemize}
        \item For a dataset \(X\) with \(n\) samples, each bootstrapped dataset \(X_i\) contains \(n\) samples drawn randomly with replacement.
        \item Final prediction: \(\hat{y} = \frac{1}{T} \sum_{i=1}^{T} \hat{y}_i\), where \(T\) is the number of models.
    \end{itemize}
    \item \textbf{Additional Concept:} 
    \begin{itemize}
        \item Bagging diversifies the training data by creating bootstrapped datasets (sampling with replacement) and training the models in parallel.
        \item This usually helps reduce overfitting and increases the robustness of the model.
    \end{itemize}
\end{itemize}

\subsection{Boosting}

\begin{itemize}
    \item \textbf{Definition:} A sequential process where each new model focuses on correcting errors made by previous models.
    \item \textbf{Mathematical Detail:}
    \begin{itemize}
        \item Weighted error: \(e_m = \frac{\sum (w_i \cdot I(y_i \neq \hat{y}_i^m))}{\sum w_i}\).
        \item Update weights: \(w_i \leftarrow w_i \cdot \exp(\alpha_m \cdot I(y_i \neq \hat{y}_i^m))\), where \(\alpha_m\) is a model's performance weight.
    \end{itemize}
    \item \textbf{Additional Concept:} Boosting helps reduce underfitting by combining a sequence of weak learners into a strong learner; however, it does little to combat overfitting if not properly regularized.
    \item \textbf{Real-World Example:} Teaching Assistants focusing on students' weak topics.
\end{itemize}

\subsection{Stacking}

\begin{itemize}
    \item \textbf{Definition:} Combines predictions from multiple models using a meta-model.
    \item \textbf{Mathematical Detail:}
    \begin{itemize}
        \item Base models provide predictions \(\hat{y}_i\). A meta-model combines these predictions: \(\hat{y}_{\text{final}} = g(\hat{y}_1, \hat{y}_2, \ldots, \hat{y}_n)\).
    \end{itemize}
    \item \textbf{Additional Concept:} Instead of using simple voting to aggregate the predictions, stacking trains a new classifier that takes the outputs from the base classifiers as inputs. The final prediction is the output of this meta-classifier.
    \item \textbf{Real-World Example:} Combining recommendations from friends, online reviews, and critics for a movie choice.
\end{itemize}

\section{General Concepts in Ensemble Learning}

\begin{itemize}
    \item The idea that groups of separate predictors work better than any individual predictor.
    \item For ensembles to work effectively, the predictors should be completely independent.
    \item The errors made by the individual predictors should be uncorrelated.
    \item Independence can be introduced by:
    \begin{itemize}
        \item Training different types of classifiers on the same dataset.
        \item Training the same type of classifier on different datasets.
    \end{itemize}
    \item The collection of these predictors is called an \textbf{ensemble}.
    \item Aggregating the predictions of the different classifiers can be done via:
    \begin{itemize}
        \item \textbf{Voting:}
        \begin{itemize}
            \item \textbf{Hard Voting:} Predict the majority class (classification) or the average prediction (regression) from the different classifiers.
            \item \textbf{Soft Voting:} If classifiers predict posterior class probabilities, predict the class with the highest average probability across all classifiers. Soft voting often outperforms hard voting.
        \end{itemize}
    \end{itemize}
\end{itemize}

\section{Application: Random Forest}

\begin{itemize}
    \item A Random Forest is an ensemble of decision trees that introduces additional randomness:
    \begin{itemize}
        \item For each decision tree, a new training set is generated by sampling with replacement from the original training set (bootstrapping).
        \item This bootstrapping process creates training sets that are similar to the original but with slight variations.
    \end{itemize}
    \item After training, predictions are performed by passing the sample through all the trees:
    \begin{itemize}
        \item For classification, the final prediction is the "majority vote" of all trees.
        \item For regression, the final prediction is the average of the predictions from all trees.
    \end{itemize}
    \item To further increase diversity, at each decision node a random subset of features is chosen for splitting rather than using the best overall feature.
\end{itemize}

\section{Application: Boosted Trees}

\begin{itemize}
    \item Boosted trees are similar to Random Forests in that they are based on decision trees, but they differ in how the training sets are created:
    \begin{itemize}
        \item Instead of sampling all examples with equal probability, misclassified examples from previously trained trees are given a higher probability of being selected.
        \item On each iteration, a new tree is trained on a bootstrapped dataset that emphasizes the examples the current ensemble struggles with.
    \end{itemize}
    \item The process of building each new tree to correct the shortcomings of the previous weak learners is called \textbf{boosting}.
    \item A popular algorithm that implements boosted trees is \textbf{XGBoost}, known for its speed and built-in regularization to prevent overfitting.
\end{itemize}

\section{Key Algorithms Using Ensemble Methods}

\subsection{Random Forest (Bagging Variant)}

\begin{itemize}
    \item \textbf{Definition:} An ensemble of decision trees, each trained on a bootstrapped dataset and a random subset of features.
    \item \textbf{Mathematical Detail:}
    \begin{itemize}
        \item At each split: Select \(\sqrt{d}\) (for classification) or \(d/3\) (for regression) features randomly from \(d\) total features.
    \end{itemize}
    \item \textbf{Real-World Example:} Predicting house prices by averaging predictions from multiple decision trees trained on different neighborhood data subsets.
\end{itemize}

\subsection{Gradient Boosting Machines (GBM)}

\begin{itemize}
    \item \textbf{Definition:} Builds models sequentially, minimizing a loss function.
    \item \textbf{Mathematical Detail:}
    \begin{itemize}
        \item New model: \( f_{m+1}(x) = f_m(x) + \eta \cdot g_m(x) \), where \(g_m(x)\) is the gradient of the loss function.
    \end{itemize}
    \item \textbf{This is how neural networks are usually trained!}
\end{itemize}

\section{Strengths and Weaknesses of Ensemble Methods}

\subsection{Strengths}

\begin{itemize}
    \item \textbf{Increased Accuracy:} Reduces overfitting and variance.
    \item \textbf{Versatility:} Works well with various data types and models.
    \item \textbf{Robustness:} Less sensitive to outliers.
\end{itemize}

\subsection{Weaknesses}

\begin{itemize}
    \item \textbf{Complexity:} Difficult to interpret.
    \item \textbf{Resource Intensive:} Higher computational cost.
    \item \textbf{Risk of Overfitting:} Especially in boosting if not regularized.
\end{itemize}

\section{Real-World Applications}

\begin{itemize}
    \item \textbf{Fraud Detection:} Combining models to flag suspicious transactions.
    \item \textbf{Medical Diagnosis:} Aggregating outputs from various diagnostic tools.
    \item \textbf{Recommendation Systems:} Leveraging user preferences, historical data, and trends.
\end{itemize}

\section{Additional Concepts and Conditions for Effective Ensembles}

\subsection{Law of Large Numbers}

\begin{itemize}
    \item \textbf{Principle:} The average of a large number of independent random variables tends toward the expected value.
    \item \textbf{Explanation:} In ensembles, averaging predictions reduces variance.
    \item \textbf{Formula:} \(\frac{1}{m} \sum X_i \rightarrow E[X]\) as \(m \rightarrow \infty\), where \(X_i\) are independent and identically distributed random variables.
\end{itemize}

\subsection{Conditions for Effective Ensembles}

\begin{itemize}
    \item \textbf{Low Bias:} Base learners should have low bias.
    \item \textbf{Uncorrelated Errors:} Errors among base learners should be uncorrelated.
    \item \textbf{Diversity:} Introduced via different model types or training on varied datasets.
\end{itemize}

\section{Limitations and Practical Considerations}

\subsection{Limitations of Ensembles}

\begin{itemize}
    \item \textbf{Computational Complexity:} Increased resource usage.
    \item \textbf{Reduced Interpretability:} Harder to understand than single models.
    \item \textbf{Tuning Required:} Balancing bias and variance demands careful tuning.
\end{itemize}

\subsection{Practical Considerations}

\begin{itemize}
    \item \textbf{Choice of Method:} Use bagging for high-variance models; boosting for high-bias models.
    \item \textbf{Efficiency:} Bagging can be parallelized; boosting is inherently sequential.
    \item \textbf{Overfitting Risks:} Boosting is more prone to overfitting; use regularization or early stopping to mitigate.
\end{itemize}

\section{Conclusion}

\begin{itemize}
    \item Decision Trees are simple and interpretable but prone to overfitting.
    \item Ensemble methods improve performance by combining multiple models.
    \item Techniques such as bagging and boosting leverage different strategies to reduce variance and bias.
    \item The effectiveness of ensembles depends on the diversity and independence of base learners.
\end{itemize}

\end{document}
