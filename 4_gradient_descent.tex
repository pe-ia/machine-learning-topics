\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb}
\usepackage{enumitem}
\usepackage{geometry}
\geometry{a4paper, margin=1in}

\title{Gradient Descent and Its Applications in Machine Learning}
\author{}
\date{}

\begin{document}

\maketitle

\section{Introduction to Gradient Descent}

\subsection{Definition and Purpose}
\begin{itemize}
    \item \textbf{Gradient Descent Definition:} Gradient descent is an optimization algorithm used to minimize a function by iteratively moving in the direction of steepest descent (the negative gradient). It is essential for training a wide variety of models, ranging from linear regression to deep neural networks.
    \item \textbf{Mathematical Detail:} If we have a function \( f(\theta) \), the update rule is:
    \[
    \theta_{t+1} = \theta_t - \eta \cdot \nabla f(\theta_t),
    \]
    where \(\eta\) (also denoted as \(\alpha\)) is the learning rate.
    \item \textbf{Goal:} Given a cost function \(J(w_1, w_2, \ldots, b)\), the aim of gradient descent is to find parameters that minimize \(J\). A lower cost indicates a smaller model error.
\end{itemize}

\subsection{Real-World Analogy}
\begin{itemize}
    \item Imagine standing on a hilly terrain in the dark, trying to find the lowest point. You take small steps in the direction that goes “downhill” (the negative gradient at your current position) until you can no longer descend.
\end{itemize}

\subsection{Gradient Descent Basics}
\begin{itemize}
    \item \textbf{Iterative Optimization:} The algorithm refines parameters through repeated steps, each moving against the gradient.
    \item \textbf{Key Update Expression:} \(\theta_{t+1} = \theta_t - \eta \nabla f(\theta_t)\).
    \item \textbf{Wide Applicability:} From simple linear regression to complex deep neural networks, gradient descent is a cornerstone of model training.
\end{itemize}


\section{Running Example: Predicting Housing Prices with Linear Regression}

Throughout this document, we will reference a simple linear regression example for predicting housing prices based on a single feature (e.g., house size).

\begin{itemize}
    \item \textbf{Model Assumption:} 
    \[
    f_{w,b}(x) = w x + b,
    \]
    where \(x\) is the size of the house, \(w\) is the weight (or slope), and \(b\) is the bias (or intercept).
    \item \textbf{Cost Function:}
    \[
    J(w,b)=\frac{1}{2m}\sum_{i=1}^{m}\left(f_{w,b}\bigl(x^{(i)}\bigr)-y^{(i)}\right)^2,
    \]
    where \(m\) is the number of training examples, and \((x^{(i)}, y^{(i)})\) denotes the training data (house size and corresponding price).
    \item \textbf{Goal:} Find \(w\) and \(b\) that minimize \(J(w,b)\) so that the predicted price is as close as possible to the actual price.
\end{itemize}

\section{Algorithm Outline and Mechanics}

\subsection{Step-by-Step Procedure with Explanations}

\begin{itemize}
    \item \textbf{Initialization:}
    \begin{itemize}[label=\(\bullet\)]
        \item \textbf{Choose initial parameter values:} For example, set \(w = 0\) and \(b = 0\). These are our starting guesses.
        \item \textbf{Note:} In practical applications, parameters may be initialized with random values to avoid symmetric behavior.
    \end{itemize}

    \item \textbf{Calculate Gradients:}
    \begin{itemize}[label=\(\bullet\)]
        \item \textbf{Definition:} The gradient of the cost function \(J(w,b)\) with respect to each parameter tells us the direction to adjust the parameters to decrease \(J\).
        \item For the housing price example, the gradients are computed as:
        \[
        \frac{\partial J(w,b)}{\partial w} = \frac{1}{m} \sum_{i=1}^{m} \underbrace{\Bigl(f_{w,b}(x^{(i)}) - y^{(i)}\Bigr)}_{\text{Error for example }i} \, x^{(i)},
        \]
        \[
        \frac{\partial J(w,b)}{\partial b} = \frac{1}{m} \sum_{i=1}^{m} \underbrace{\Bigl(f_{w,b}(x^{(i)}) - y^{(i)}\Bigr)}_{\text{Error for example }i}.
        \]
        \begin{itemize}[label=\(\bullet\)]
            \item Here, \(m\) is the total number of training examples.
            \item The term \(f_{w,b}(x^{(i)})\) represents the prediction for the \(i\)th training example.
            \item The term \(y^{(i)}\) is the true value for the \(i\)th training example.
            \item The difference \(f_{w,b}(x^{(i)}) - y^{(i)}\) is the error for that example.
            \item Multiplying by \(x^{(i)}\) in the gradient for \(w\) accounts for the influence of the feature on the prediction error.
        \end{itemize}
    \end{itemize}

    \item \textbf{Iterative Updates:}
    \begin{itemize}[label=\(\bullet\)]
        \item \textbf{Update Rule:} At each iteration, parameters are updated using the formula:
        \[
        w \leftarrow w - \alpha \frac{\partial J(w,b)}{\partial w}, \qquad
        b \leftarrow b - \alpha \frac{\partial J(w,b)}{\partial b}.
        \]
        \item \(\alpha\) (or the learning rate) controls the size of the step taken in the negative gradient direction.
        \item \textbf{Interpretation:} The update subtracts a proportion (\(\alpha\)) of the gradient, thereby shifting \(w\) and \(b\) towards lower cost.
    \end{itemize}

    \item \textbf{Simultaneous Parameter Updates:}
    \begin{itemize}[label=\(\bullet\)]
        \item \textbf{Key Point:} Compute all parameter gradients using the values from the previous iteration.
        \item Then update all parameters \emph{together}. This ensures consistency, meaning every update reflects the same state of parameters before any of them change.
    \end{itemize}

    \item \textbf{Convergence Check:}
    \begin{itemize}[label=\(\bullet\)]
        \item \textbf{Stopping Conditions:}
        \begin{itemize}[label=\(\bullet\)]
            \item Stop when the change in the cost function \(J(w,b)\) is smaller than a defined threshold.
            \item Alternatively, stop if the magnitude of the gradient is less than a small value, indicating little change in subsequent updates.
            \item Or, stop after a maximum number of iterations.
        \end{itemize}
        \item \textbf{Practical Note:} Monitoring the cost function or the gradient magnitude provides insights into whether the algorithm is converging.
    \end{itemize}
\end{itemize}

\subsection{Learning Rate Considerations}

\begin{itemize}
    \item \(\alpha\) (learning rate) controls the size of parameter updates.
    \item \textbf{Too Small:} Convergence may be accurate but very slow.
    \item \textbf{Too Large:} Updates might overshoot, causing divergence or oscillations.
    \item \textbf{Practical Tip:} In the housing price example, choose \(\alpha\) carefully. If the cost bounces wildly, lower \(\alpha\). If convergence is glacially slow, increase \(\alpha\).
\end{itemize}

\subsection{Diagnosing Convergence and Performance}

\begin{itemize}
    \item \textbf{Learning Curve:} Plot the cost function vs.\ the number of iterations. A smoothly decreasing curve that flattens near the end indicates proper convergence.
    \item \textbf{Cost Fluctuations:} Large fluctuations may signal that \(\alpha\) is too high.
    \item \textbf{Stopping Criteria:}
    \begin{itemize}[label=\(\bullet\)]
        \item Gradient magnitude falls below a certain threshold.
        \item Change in the cost function is less than a small value \(\epsilon\).
        \item A maximum number of iterations is reached.
    \end{itemize}
\end{itemize}

\section{Types of Gradient Descent}

\subsection{Batch Gradient Descent}
\begin{itemize}
    \item \textbf{Definition:} Uses the entire training dataset to compute the gradient at each update.
    \item \textbf{Advantages:} Generally stable in convex problems and produces less noisy updates.
    \item \textbf{Drawbacks:} Computationally heavy for large datasets, which can slow down each iteration.
    \item \textbf{Example (Housing Prices):} You compute the cost and its gradient over all houses in your dataset before making each update to \(w\) and \(b\).
\end{itemize}

\subsection{Stochastic Gradient Descent (SGD)}
\begin{itemize}
    \item \textbf{Definition:} Updates parameters for each individual training example (or for a small subset of size 1).
    \item \textbf{Advantages:} Fast updates, can help the model jump out of local minima in non-convex scenarios.
    \item \textbf{Drawbacks:} The gradient estimate is noisy, which can cause erratic convergence.
    \item \textbf{Example (Housing Prices):} Pick one house at random, compute the gradient, then update \(w\) and \(b\).
\end{itemize}

\subsection{Mini-Batch Gradient Descent}
\begin{itemize}
    \item \textbf{Definition:} Splits the dataset into small batches. Each batch gradient is used to update parameters.
    \item \textbf{Advantages:} Balances speed (compared to pure batch) and stability (compared to pure SGD).
    \item \textbf{Common Practice:} Widely used in modern machine learning frameworks (like TensorFlow and PyTorch) for efficient training.
    \item \textbf{Example (Housing Prices):} You group your houses in mini-batches of size, say, 32 or 64, compute the gradient on each batch, and perform an update.
\end{itemize}

\section{Applications in Machine Learning}

\subsection{Linear Regression}
\begin{itemize}
    \item \textbf{Model:} 
    \[
    f_{w,b}(x) = wx + b,
    \]
    where:
    \begin{itemize}[label=\(\bullet\)]
        \item \(w\) is the weight (slope),
        \item \(b\) is the bias (intercept),
        \item \(x\) is the input feature (for example, the size of a house in our housing price prediction example).
    \end{itemize}

    \item \textbf{Cost Function:}
    \[
    J(w,b)=\frac{1}{2m}\sum_{i=1}^{m}\underbrace{\Bigl(f_{w,b}(x^{(i)})-y^{(i)}\Bigr)^2}_{\text{Squared error for example }i},
    \]
    where:
    \begin{itemize}[label=\(\bullet\)]
        \item \(m\) is the number of training examples,
        \item \(f_{w,b}(x^{(i)})\) is the predicted value for the \(i\)th example,
        \item \(y^{(i)}\) is the actual value for the \(i\)th example.
    \end{itemize}

    \item \textbf{Gradient Updates:}
    \begin{itemize}[leftmargin=*, label=\(\bullet\)]
        \item For the weight \(w\):
        \[
        \frac{\partial J(w,b)}{\partial w}=\frac{1}{m}\sum_{i=1}^{m} \underbrace{\Bigl(f_{w,b}(x^{(i)})-y^{(i)}\Bigr)}_{\text{Error for example }i} \, \cdot \underbrace{x^{(i)}}_{\substack{\text{Input feature} \\ \text{(contributing factor)}}},
        \]
        \item For the bias \(b\):
        \[
        \frac{\partial J(w,b)}{\partial b}=\frac{1}{m}\sum_{i=1}^{m} \underbrace{\Bigl(f_{w,b}(x^{(i)})-y^{(i)}\Bigr)}_{\text{Error for example }i}.
        \]
    \end{itemize}

    \item \textbf{Application:} Our housing price example is a canonical use case, where we adjust \(w\) and \(b\) to minimize the prediction error for the house prices based on the input feature \(x\) (house size).
\end{itemize}

\subsection{Logistic Regression}
\begin{itemize}
    \item \textbf{Task:} Binary classification, such as deciding whether an email is spam or not spam.
    \item \textbf{Loss Function (Binary Cross-Entropy):}
    \[
    L(\theta) = -\frac{1}{m}\sum_{i=1}^{m}\underbrace{\Bigl[y_i\log (\hat{y}_i)+(1-y_i)\log (1-\hat{y}_i)\Bigr]}_{\text{Error term for example }i},
    \]
    where:
    \begin{itemize}[label=\(\bullet\)]
        \item \(y_i\) is the true label for the \(i\)th example (either 0 or 1),
        \item \(\hat{y}_i\) is the predicted probability of the positive class,
        \item \(m\) is the number of examples.
    \end{itemize}

    \item \textbf{Gradient Updates:}
    \begin{itemize}[label=\(\bullet\)]
        \item Similar to linear regression, calculate the partial derivatives of the loss function \(L(\theta)\) with respect to each parameter (weights and bias).
        \item For each parameter \(\theta_j\), the update rule is:
        \[
        \theta_j \leftarrow \theta_j - \alpha \, \frac{\partial L(\theta)}{\partial \theta_j},
        \]
        where \(\alpha\) is the learning rate.
        \item The expression inside the summation (under the brace) represents the contribution of each training example's error to the overall loss, and it is used for computing the gradient.
    \end{itemize}
\end{itemize}


\subsection{Neural Networks}
\begin{itemize}
    \item \textbf{Backpropagation:} Gradient descent is used in conjunction with backpropagation to train multi-layer networks.
    \item \textbf{Loss Functions:} Often cross-entropy or mean squared error, depending on the task.
    \item \textbf{Example:} Training a convolutional neural network for image classification typically involves mini-batch gradient descent with large datasets.
\end{itemize}

\section{Challenges \& Considerations}

\subsection{Learning Rate (\(\alpha\))}
\begin{itemize}
    \item \textbf{Fine-Tuning:} Choosing \(\alpha\) is crucial for successful convergence.
    \item \textbf{Too Small:} Convergence is very slow.
    \item \textbf{Too Large:} Risk of overshooting the minimum, causing divergence or oscillation.
\end{itemize}

\subsection{Local Minima and Saddle Points}
\begin{itemize}
    \item \textbf{Non-Convex Problems:} In complex models (e.g., deep neural networks), gradient descent can get stuck in local minima or plateaus (saddle points).
    \item \textbf{Practice:} Stochastic methods and random restarts sometimes help escape poor local minima.
\end{itemize}

\subsection{Ill-Conditioned Loss Surfaces}
\begin{itemize}
    \item \textbf{Cliffs \& Valleys:} Certain directions may be steep while others are flat, making gradient updates inefficient.
    \item \textbf{Feature Scaling:} Normalizing or standardizing features can improve the shape of the loss surface and speed up convergence.
\end{itemize}

\subsection{Convergence Diagnosis}
\begin{itemize}
    \item \textbf{Plot Cost vs.\ Iterations:} Helps verify that the cost steadily decreases.
    \item \textbf{Check for Oscillation or Divergence:} May indicate a learning rate that is too large.
\end{itemize}

\section{Stopping Criteria and Diagnostics}

\begin{itemize}
    \item \textbf{Convergence Criteria:}
    \begin{itemize}[label=\(\bullet\)]
        \item The gradient magnitude falls below a predetermined threshold.
        \item The change in cost \(\Delta J\) is less than a small value \(\epsilon\).
        \item A maximum number of iterations is reached.
    \end{itemize}
    \item \textbf{Visual Diagnostics:} Plotting the cost vs.\ the number of iterations is a common practice to check descent progression.
    \item \textbf{Parameter Updates:} Always ensure all parameters are updated \emph{simultaneously} using gradients computed from the same iteration.
\end{itemize}

\section{Advanced Extensions}

\subsection{Momentum}
\begin{itemize}
    \item \textbf{Concept:} Accelerates gradient descent by adding a fraction of the previous update to the current one, helping overcome small local minima and reduce oscillations.
    \item \textbf{Update Rule:}
    \[
    v(t) = \gamma \cdot v(t-1) + \eta \cdot \nabla f(\theta), 
    \quad 
    \theta = \theta - v(t),
    \]
    where \(\gamma\) is the momentum coefficient.
\end{itemize}

\subsection{Adaptive Learning Rates}
\begin{itemize}
    \item \textbf{AdaGrad:} Adapts the learning rate based on the cumulative sum of squared gradients.
    \item \textbf{RMSProp:} Uses an exponentially decaying average of squared gradients to adapt the learning rate.
    \item \textbf{Adam:} Combines momentum (via moving averages of the gradient and its square) with adaptive learning rates for robust performance.
\end{itemize}

\subsection{Second-Order Methods}
\begin{itemize}
    \item \textbf{Newton's Method:} Leverages second derivatives (the Hessian) for potentially faster convergence, although it is computationally expensive.
    \item \textbf{Quasi-Newton Methods (e.g., BFGS):} Approximate the Hessian to balance computational cost and convergence speed.
\end{itemize}

\section{Additional Real-World Examples}

\begin{itemize}
    \item \textbf{Online Learning:} In real-time systems such as recommender platforms, stochastic gradient descent (SGD) is used to update parameters continually as new data arrives.
    \item \textbf{Distributed Systems:} Mini-batch gradient descent can be parallelized across multiple machines (e.g., in TensorFlow or PyTorch), speeding up training on large datasets.
    \item \textbf{Financial Modeling:} Adaptive methods like Adam help optimize risk-return profiles by dynamically tuning learning rates.
\end{itemize}

\section{Conclusion}

\begin{itemize}
    \item \textbf{Summary:} Gradient descent is a foundational algorithm for minimizing cost functions in machine learning. It proceeds by stepping against the gradient of the objective function.
    \item \textbf{Flexible Variants:} Batch, stochastic, and mini-batch gradient descent each have distinct trade-offs between computational efficiency and update stability.
    \item \textbf{Key Considerations:} Proper learning rate selection, convergence diagnostics, and addressing local minima issues are crucial for successful training.
    \item \textbf{Advanced Extensions:} Techniques like momentum, adaptive learning rates (AdaGrad, RMSProp, Adam), and second-order methods can significantly improve performance.
    \item \textbf{Final Thought:} Despite its simplicity, gradient descent underpins most modern machine learning optimization strategies, including our ongoing housing price prediction example and beyond.
\end{itemize}

\end{document}