\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{a4paper, margin=1in}

\title{Neural Networks}
\author{}
\date{}

\begin{document}

\maketitle

\section{Introduction}

\begin{itemize}
    \item A neural network consists of neurons arranged in layers.
    \item Each neuron:
    \begin{itemize}
        \item Receives input signals.
        \item Processes data through a weighted combination, adjusted with a bias.
        \item Applies a non-linear activation function.
    \end{itemize}
    \item This process is inspired by the brain's method for processing sensory inputs.
    \item Just like brains, neural networks can detect complex patterns, even from raw data (i.e. pixels)
\end{itemize}

\subsection{Mathematical Detail}

\begin{itemize}
    \item \textbf{Neuron Computation:}
    \begin{itemize}
        \item Compute the weighted sum:
        \[
        z = \sum_{i} w_i x_i + b
        \]
        \begin{itemize}
            \item \(w_i\): Weights representing the importance of each input.
            \item \(x_i\): Input values.
            \item \(b\): Bias term that shifts the activation function.
        \end{itemize}
        \item Apply a non-linear activation:
        \[
        a = f(z)
        \]
        \begin{itemize}
            \item \(f(z)\): A non-linear function (e.g., ReLU: \(\max(0, z)\)).
            \item Introduces non-linearity so the network can learn complex patterns.
        \end{itemize}
    \end{itemize}
\end{itemize}

\section{Structure of Neural Networks}

\begin{itemize}
    \item \textbf{Three Key Layers:}
    \begin{enumerate}
        \item \textbf{Input Layer:}
        \begin{itemize}
            \item Receives raw data.
            \item No complex computations are performed.
        \end{itemize}
        \item \textbf{Hidden Layers:}
        \begin{itemize}
            \item Perform transformations via weighted sums and activations.
            \item Extract and refine features from the input.
            \item \textbf{Universal Approximation Theorem:}
             \begin{itemize}
                \item A single hidden layer with sufficient neurons can approximate any continuous function.
                \item Requires non-linear activations.
            \end{itemize}
        \end{itemize}
        \item \textbf{Output Layer:}
        \begin{itemize}
            \item Produces the final prediction.
            \item Uses appropriate activations (e.g., softmax for multi-class tasks, sigmoid for binary tasks).
        \end{itemize}
    \end{enumerate}
    
    \item \textbf{Analogy:} Think of the layers as an assembly line:
    \begin{itemize}
        \item Input layer: Receives raw materials.
        \item Hidden layers: Process and refine the materials.
        \item Output layer: Delivers the finished product.
    \end{itemize}
    
    \item \textbf{Parameters vs. Hyperparameters:}
    \begin{itemize}
        \item \textbf{Parameters:}
        \begin{itemize}
            \item Weights (\(w_i\)) and biases (\(b\)).
            \item Learned automatically during training.
        \end{itemize}
        \item \textbf{Hyperparameters:}
        \begin{itemize}
            \item Manually set (e.g., learning rate for gradient descent, number of hidden layers).
        \end{itemize}
    \end{itemize}
    
    \item \textbf{Gradient Descent Update:}
    \begin{itemize}
        \item Update rule:
        \[
        \theta \leftarrow \theta - \eta \frac{\partial J}{\partial \theta}
        \]
        \begin{itemize}
            \item \(\theta\): Any weight or bias.
            \item \(J\): Loss function.
            \item \(\eta\): Learning rate controlling update size.
        \end{itemize}
    \end{itemize}
\end{itemize}

\section{Training Neural Networks}

\begin{enumerate}
        \item \textbf{Forward Propagation:}
        \begin{itemize}
            \item Input data is passed through each layer.
            \item At each neuron:
            \[
            z = \sum_{i} w_i x_i + b \quad \underbrace{\text{Linear combination of inputs, weights, and bias}}_{\text{Weighted sum}}
            \]
            \[
            a = f(z) \quad \underbrace{\text{Non-linear activation function}}_{\text{e.g., ReLU, Sigmoid}}
            \]
            Then $a$ is passed as the input to the next layer's neurons
            \item The final prediction is generated at the output layer.
        \end{itemize}
        
        \item \textbf{Loss:} Compute the loss using functions such as:
        \begin{itemize}
            \item Mean Squared Error (MSE):
            \[
            J = \frac{1}{n} \sum (y_{\text{true}} - y_{\text{pred}})^2 \quad \underbrace{\text{Measures average squared error}}_{\text{Regression tasks}}
            \]
            \item Binary Cross-Entropy:
            \[
            J = -\bigl[y \log(a_o) + (1-y) \log(1-a_o)\bigr] \quad \underbrace{\text{Evaluates prediction probability}}_{\text{Classification tasks}}
            \]
        \end{itemize}
        
    \item \textbf{Backpropagation and Gradient Descent:}
    \begin{itemize}
        \item \textbf{Intuitive Overview:}
            \begin{itemize}
                \item \textbf{Backpropagation} is the process used to compute how much each parameter in the network (weights and biases) contributed to the error in the output.
                \item It works by moving backward through the network, calculating the gradient (or derivative) of the loss with respect to each parameter, effectively answering the question: \textit{"How should each parameter change to reduce the error?"}
                \item \textbf{Gradient Descent} then uses these gradients to update the parameters, ensuring the network learns by taking small steps in the direction that minimizes the loss.
            \end{itemize}

        \pagebreak
        
        \item \textbf{Computing the Gradients:}
    \begin{itemize}
        \item Our goal is to minimize the loss \(J\), which measures the difference between the model's predictions and the true labels.
        \item The chain rule is used to compute the gradient of \(J\) with respect to the parameters \(\theta\) (weights \(w\) and biases \(b\)):
        \[
        \frac{\partial J}{\partial \theta} = \frac{\partial J}{\partial a} \cdot \frac{\partial a}{\partial z} \cdot \frac{\partial z}{\partial \theta}.
        \]
        \begin{itemize}
            \item \(\frac{\partial J}{\partial a}\): Measures how sensitive the loss is to the output \(a\) of the neuron.
            \item \(\frac{\partial a}{\partial z}\): Derivative of the activation function \(f(z)\) (e.g., sigmoid, ReLU, or softmax).
            \item \(\frac{\partial z}{\partial \theta}\): How the weighted sum \(z = \sum w x + b\) changes with respect to the parameters \(w\) and \(b\).
        \end{itemize}
        \item For specific loss functions and activation functions:
        \begin{itemize}
            \item For sigmoid + binary cross-entropy: \(\frac{\partial J}{\partial a} = a - y\).
            \item For softmax + cross-entropy: \(\frac{\partial J}{\partial a_c^{(o)}} = a_c^{(o)} - y_c\).
        \end{itemize}
    \end{itemize}
        
            \item \textbf{Adjusting the Parameters:}
                \begin{itemize}
                    \item After computing \(\frac{\partial J}{\partial \theta}\) for every parameter using backpropagation, gradient descent updates each parameter to reduce the loss.
                    \item The update rule is given by:
                        \[
                        \theta \leftarrow \theta - \eta \frac{\partial J}{\partial \theta}
                        \]
                        where:
                        \begin{itemize}
                            \item \(\eta\) is the learning rate, a hyperparameter that controls the step size during the update.
                            \item A \(\textbf{small \(\eta\)}\) ensures gradual learning (stable but slow), whereas a \(\textbf{large \(\eta\)}\) speeds up learning but risks overshooting the minimum of the loss function.
                        \end{itemize}
                    \item In essence, gradient descent uses the information provided by backpropagation to move the parameters \(\theta\) in the direction that most steeply decreases the loss \(J\).
                \end{itemize}
        \end{itemize}
    
\end{enumerate}

\section{Extensions to simple ANNs}

\subsection{Softmax Output for Multi-Class Classification:}
\begin{itemize}
    \item \textbf{Softmax Definition:}
    \begin{itemize}
        \item Converts raw outputs (logits) into a probability distribution.
        \item Given logits:
        \[
        \mathbf{z}^{(o)} = [z_1^{(o)}, z_2^{(o)}, \ldots, z_C^{(o)}],
        \]
        each output probability is:
        \[
        a_c^{(o)} = \frac{e^{z_c^{(o)}}}{\sum_{k=1}^{C} e^{z_k^{(o)}}} \quad,\quad c = 1, \ldots, C.
        \]
        \item \(a_c^{(o)}\) is the predicted probability for class \(c\).
    \end{itemize}
    \item \textbf{Multi-Class Cross-Entropy Loss:}
    \begin{itemize}
        \item Measures the error between predicted probabilities and true one-hot labels.
        \item Loss function:
        \[
        L(\mathbf{a}^{(o)}, \mathbf{y}) = -\sum_{c=1}^{C} y_c \, \log\bigl(a_c^{(o)}\bigr),
        \]
        where \(y_c = 1\) for the correct class and \(0\) otherwise.
    \end{itemize}
    \item Then, calculate gradients and backpropogate\end{itemize}

\subsection{Regularization Methods}

\begin{itemize}
    \item \textbf{Weight Decay (L2 Regularization):}
    \begin{itemize}
        \item \textbf{Loss with penalty:}
        \[
        Q(\mathbf{W}) + \frac{\lambda}{2}\|\mathbf{W}\|_2^2,
        \]
        where:
        \begin{itemize}
            \item \(Q(\mathbf{W})\): Original loss (e.g., cross-entropy).
            \item \(\|\mathbf{W}\|_2^2 = \sum W_{i,j}^2\): L2 norm of weights.
            \item \(\lambda > 0\): Regularization strength.
        \end{itemize}
        \item \textbf{Update rule:}
        \[
        \mathbf{W}_{t+1} = \mathbf{W}_t - \alpha \nabla_{\mathbf{W}} Q(\mathbf{W}_t) - \lambda \mathbf{W}_t,
        \]
        where:
        \begin{itemize}
            \item \(-\alpha \nabla_{\mathbf{W}} Q(\mathbf{W}_t)\): Update to reduce original loss.
            \item \(-\lambda \mathbf{W}_t\): Shrinks weights to prevent overfitting.
        \end{itemize}
        \item Regularization penalizes large weights, improving generalization.
    \end{itemize}
    \item \textbf{Dropout:}
    \begin{itemize}
        \item \textbf{Idea:} Randomly disable neurons during training.
        \item Each neuron's output is multiplied by a random mask.
        \item Inverted dropout scales outputs by \(\frac{1}{\text{keep-prob}}\) so no scaling is needed at inference.
    \end{itemize}
    \item \textbf{Early Stopping:}
    \begin{itemize}
        \item Monitor validation loss.
        \item Stop training when validation performance no longer improves.
    \end{itemize}
    \item \textbf{Data Augmentation:}
    \begin{itemize}
        \item Apply random transformations (rotations, flips, noise) to increase effective training data.
        \item Particularly useful in image-based tasks.
    \end{itemize}
\end{itemize}

\subsection{Variants of Gradient Descent}

 \begin{itemize}
    \item \textbf{Momentum:}
    \begin{itemize}
        \item Update equations:
        \[
        \begin{aligned}
        v_{t+1} &= \beta\, v_{t} + \alpha\, \nabla L(\mathbf{W}_t),\\[1mm]
        \mathbf{W}_{t+1} &= \mathbf{W}_t - v_{t+1}.
        \end{aligned}
        \]
        \item \(v_t\) accumulates a history of gradients.
    \end{itemize}
    \item \textbf{Nesterov Accelerated Gradient (NAG):}
        \begin{itemize}
            \item Update equations:
            \[
            \begin{aligned}
            v_{t+1} &= \beta\, v_t + \alpha\, \nabla L(\mathbf{W}_t \underline{+ \beta\, v_t}),\\[1mm]
            \mathbf{W}_{t+1} &= \mathbf{W}_t - v_{t+1}.
            \end{aligned}
            \]
            \item Gradients are computed based on a "look-ahead" position:
            \[
            \mathbf{W}_t + \beta\, v_t,
            \]
            anticipating the effect of the momentum step before applying it.
            \item This leads to more informed updates, potentially reducing overshooting and improving convergence.
        \end{itemize}
    \item \textbf{Other Optimizers:} 
    \begin{itemize}
        \item Adagrad, Adadelta, RMSProp, Adam.
        \item These automatically adjust the learning rate per parameter.
    \end{itemize}
\end{itemize}

\section{Deep Network Challenges and Convolutional Neural Networks}

\begin{itemize}
    \item \textbf{Weight Initialization and the Symmetry Problem:}
    \begin{itemize}
        \item \textbf{Symmetry Problem:}
        \begin{itemize}
            \item If all weights in a layer are initialized to the same value, neurons receive identical gradients.
            \item Prevents them from learning diverse features.
        \end{itemize}
        \item \textbf{Bias Initialization:}
        \begin{itemize}
            \item Biases are often initialized to zero.
            \item They shift the activation function without causing symmetry issues.
        \end{itemize}
        \item \textbf{Guidelines for Weight Initialization:}
        \begin{itemize}
            \item \textbf{Xavier (Glorot) Initialization:}
            \[
            w_{i,j} \sim \mathcal{N}\!\bigl(0,\,\sigma^2\bigr), \quad \sigma^2 = \frac{1}{n_{\text{avg}}}, \quad n_{\text{avg}} = \tfrac{1}{2}\bigl(n_{\text{in}} + n_{\text{out}}\bigr).
            \]
            \item \textbf{He Initialization:}
            \[
            \sigma^2 = \frac{2}{n_{\text{avg}}},
            \]
            \begin{itemize}
                \item Often recommended for ReLU-type activations.
            \end{itemize}
        \end{itemize}
    \end{itemize}
    
    \item \textbf{Vanishing and Exploding Gradients in Deep Networks:}
    \begin{itemize}
        \item \textbf{Vanishing Gradients:}
        \begin{itemize}
            \item \textbf{Definition:} Gradients become very small in deeper layers.
            \item \textbf{Why It Happens:}
            \begin{itemize}
                \item Saturating activations like sigmoid or tanh yield derivatives near zero for large input magnitudes.
                \item Repeated multiplication (chain rule) of small derivatives results in vanishing gradients.
            \end{itemize}
            \item \textbf{Consequence:}
            \begin{itemize}
                \item Early layers learn very slowly, reducing the network's ability to capture useful features.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Exploding Gradients:}
        \begin{itemize}
            \item \textbf{Definition:} Gradients become excessively large.
            \item \textbf{Why It Happens:}
            \begin{itemize}
                \item Large weight values and derivatives greater than 1 can amplify the gradients.
                \item Repeated multiplication can cause exponential growth.
            \end{itemize}
            \item \textbf{Consequence:}
            \begin{itemize}
                \item Causes numerical instability (NaN or infinite values) and erratic training.
            \end{itemize}
        \end{itemize}
    \end{itemize}
    
    \item \textbf{Activation Functions in Hidden Layers:}
    \begin{itemize}
        \item \textbf{Sigmoid:} 
        \[
        f(x) = \frac{1}{1 + e^{-x}}.
        \]
        \begin{itemize}
            \item Saturates at 0 or 1; leads to vanishing gradients.
            \item Involves computationally expensive exponential calculations.
        \end{itemize}
        \item \textbf{Tanh:}
        \[
        f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}.
        \]
        \begin{itemize}
            \item Ranges from -1 to 1.
            \item Steeper derivatives than sigmoid, but still suffers from saturation for large \(|x|\).
        \end{itemize}
        \item \textbf{ReLU:}
        \[
        f(x) = \max(0,\,x).
        \]
        \begin{itemize}
            \item Computationally fast; derivative is 1 for \(x > 0\), and 0 for \(x < 0\).
            \item \textbf{Dying ReLU Problem:} Neurons with negative inputs become inactive permanently.
        \end{itemize}
        \item \textbf{Leaky ReLU:}
        \[
        f(x) = 
        \begin{cases}
           x & \text{if } x \ge 0,\\[1mm]
           \alpha x & \text{if } x < 0,
        \end{cases}
        \]
        where \(0 < \alpha \ll 1\).
        \begin{itemize}
            \item Provides a small gradient when \(x < 0\) to mitigate the dying ReLU problem.
        \end{itemize}
    \end{itemize}
\end{itemize}

\section{Convolutional Neural Networks (CNNs)}

\begin{itemize}
        \item \textbf{Motivation and Overview:}
        \begin{itemize}
            \item \textbf{Spatial Structure:} Flattening images loses local information.
            \item \textbf{Convolutions:} Apply small filters (kernels) to extract local patterns (edges, corners, textures) with fewer parameters.
            \item \textbf{Multi-Channel Inputs/Outputs:} Handle images with multiple color channels (e.g., RGB) and learn several filters simultaneously.
        \end{itemize}
        
        \item \textbf{Convolution Operation (2D Case):}
        \begin{itemize}
            \item For a filter of size \(k \times k\) sliding over an image:
            \[
            \text{ConvOutput}[i,j] = \sum_{p=0}^{k-1} \sum_{q=0}^{k-1} \bigl(\text{Image}[i+p,\,j+q]\bigr) \, \bigl(\text{Filter}[p,q]\bigr).
            \]
            \item \textbf{Padding:}
            \begin{itemize}
                \item \textit{Valid Padding}: No extra border; output is smaller than the input.
                \item \textit{Same Padding}: Zeros are added so the output size is the same as the input.
            \end{itemize}
            \item \textbf{Stride:} Determines the step size when moving the filter over the image. Larger strides downsample the feature map more aggressively.
        \end{itemize}
        
        \item \textbf{Pooling:}
        \begin{itemize}
            \item \textbf{Purpose:} Downsample feature maps to reduce dimensionality and control overfitting.
            \item \textbf{Max-Pooling:} Selects the maximum value within each local region.
            \item \textbf{Example:} A \(2\times2\) max-pooling layer selects the largest value from each \(2\times2\) block.
        \end{itemize}
        
        \item \textbf{CNN Architecture:}
        \begin{itemize}
            \item \textbf{Feature Extraction:} Convolution, activation, and pooling layers stack to capture features at multiple scales.
            \item \textbf{Classification Stage:} Flatten the final feature maps and feed into fully-connected layers (often ending with softmax for multi-class classification).
        \end{itemize}
    \end{itemize}

\section{Challenges and Limitations}

\begin{itemize}
    \item \textbf{Overfitting:}
    \begin{itemize}
        \item The model may memorize training data.
        \item Regularization techniques (dropout, weight decay) are used to improve generalization.
    \end{itemize}
    
    \item \textbf{Computational Cost:}
    \begin{itemize}
        \item Training large networks requires significant computational resources.
    \end{itemize}
    
    \item \textbf{Interpretability:}
    \begin{itemize}
        \item Neural networks are often considered "black boxes." (unlike SVMs or linear regression for example)
        \item Their internal decision processes are not easily interpreted.
    \end{itemize}
\end{itemize}

\section{Applications of Neural Networks}

\begin{itemize}
    \item \textbf{Image Recognition:}
    \begin{itemize}
        \item Convolutional neural networks (CNNs) are widely used.
        \item Example operation: Convolution 
        \[
        (f * g)(t) = \int f(\tau) g(t-\tau) \, d\tau.
        \]
        \item \textbf{Explanation:}
        \begin{itemize}
            \item \(f\) represents the input image.
            \item \(g\) is the filter (kernel) that extracts features.
        \end{itemize}
        \item Real-world example: Face unlock systems in smartphones.
    \end{itemize}
    
    \item \textbf{Natural Language Processing (NLP):}
    \begin{itemize}
        \item Uses RNNs and transformers.
        \item \textbf{Key Component:} The attention mechanism:
        \[
        \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V.
        \]
        \item \textbf{Explanation:}
        \begin{itemize}
            \item \(Q\): Query matrix.
            \item \(K\): Key matrix.
            \item \(V\): Value matrix.
            \item The dot product is scaled and normalized to create a probability distribution.
        \end{itemize}
        \item Example: Machine translation (e.g., Google Translate).
    \end{itemize}
\end{itemize}

\end{document}
